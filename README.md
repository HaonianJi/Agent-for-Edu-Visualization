# LM-MultiAgent-Framework for Educational Content Generation

## Overview

This project implements a multi-agent system designed to generate comprehensive educational content based on user queries. It leverages a series of specialized Language Model (LM) agents that collaborate to create detailed teaching plans, refine them through iterative feedback, and structure them for various presentation formats. The core of this system is the `TeachAgentsIntermediateSystem`, which orchestrates the entire workflow.

## Features

*   **Multi-Agent Collaboration:** Utilizes a suite of specialized agents (e.g., Visual Representer, Information Expresser, Cognitive Strategist, Metacognitive Strategist, Schema Instruction Specialist, Web Page Formatting Specialist) to contribute diverse expertise.
*   **Structured Educational Content:** Generates teaching plans broken down into logical parts, each addressing specific learning objectives.
*   **Iterative Refinement:** Employs a refinement loop where specialist agents provide feedback on each part of the plan, and a synthesizer agent incorporates this feedback to improve the content.
*   **Detailed Output Logging:** Saves all intermediate and final outputs in a well-organized, timestamped directory structure for traceability and review. This includes:
    *   The initial user query.
    *   The raw initial teaching plan.
    *   Each extracted part from the initial plan.
    *   For each part during refinement:
        *   The original content.
        *   Feedback from each specialist agent.
        *   Combined feedback.
        *   The refined content after synthesis.
    *   The final combined refined plan.
    *   Separated final refined parts.
    *   UI/Presentation plans for each part.
*   **Configurable Agents and Models:** Agent behaviors and LM model parameters can be configured through YAML files.
*   **OpenAI Integration:** Currently uses OpenAI models (e.g., GPT-4o) for language generation tasks.

## Directory Structure

```
LM-MultiAgent-Framework/
├── agents/                         # Core agent logic
│   ├── teach_agents_intermediate_system.py # Main orchestrator
│   └── base_agent.py               # Base class for all agents
├── config/                         # Configuration files
│   ├── agent/                      # Individual agent configurations (YAML)
│   ├── model/                      # Language model configurations (YAML)
│   └── multi_agents/               # Multi-agent system configurations (YAML)
│       └── education.yaml          # Example: education query processing setup
├── models/                         # Language model interface
│   └── openai_model.py             # OpenAI API integration
├── outputs/                        # Generated outputs from runs
│   └── teach_intermediate/         # Outputs for the educational query system
│       └── [timestamped_run_dir]/  # Specific run outputs
├── scripts/                        # Executable scripts
│   └── run_teach_intermediate.py   # Main script to run the system
├── .env.example                    # Example environment file for API keys
├── requirements.txt                # Python dependencies
└── README.md                       # This file
```

## Setup

1.  **Clone the repository (if applicable):**
    ```bash
    git clone https://github.com/HaonianJi/Agent-for-Edu-Visualization.git
    cd LM-MultiAgent-Framework
    ```

2. **Create and activate a Conda environment (recommended):**

   ```bash
   conda create -n lm-maf python=3.12
   conda activate lm-maf
   ```

3. **Install dependencies:**

   ```bash
   bash install.sh
   ```


4.  **Set up Environment Variables:**
    Copy the `.env.example` file to `.env` and add your OpenAI API key:
    ```bash
    cp .env.example .env
    ```
    Then edit `.env`:
    ```
    OPENAI_API_KEY="your_openai_api_key_here"
    ```

## Usage

To run the educational content generation system, execute the main script:

```bash
python scripts/run_teach_intermediate.py
```

By default, this script uses a predefined sample query. You can modify the query within the script if needed.

Outputs will be saved in the `outputs/teach_intermediate/` directory, under a subdirectory named with the timestamp and a slug of the query (e.g., `outputs/teach_intermediate/YYYYMMDD_HHMMSS_explain_concept_photosynthesis/`).

### Understanding the Output Structure

Inside each run's output directory, you will find:

*   `00_user_query.txt`: The initial query provided to the system.
*   `01_initial_plan_raw.txt`: The first teaching plan generated by the planning agents.
*   `02_initial_parts_extracted/`: Directory containing each part extracted from `01_initial_plan_raw.txt`.
    *   `part_1.txt`, `part_2.txt`, ...
*   `03_refinement_process/`: Directory containing the detailed refinement steps for each part.
    *   `part_1/`
        *   `01_original_content.txt`: The content of this part before refinement.
        *   `02_feedback_visual_representer.txt`: Feedback from the Visual Representer.
        *   `03_feedback_info_expresser.txt`: Feedback from the Information Expresser.
        *   ... (other specialist feedback files)
        *   `06_combined_feedback_for_synthesizer.txt`: All feedback combined.
        *   `07_refined_content_by_schema_instruction_specialist.txt`: The refined content for this part.
    *   `part_2/`, ... (similar structure for other parts)
*   `04_final_refined_plan_combined.txt`: The complete teaching plan after all parts have been refined, combined into a single file.
*   `05_final_refined_parts_separated/`: Directory containing the final refined content for each part, saved as separate files.
    *   `part_1_refined.txt`, `part_2_refined.txt`, ...
*   `06_ui_plans_for_each_part/`: Directory containing UI/presentation plans generated for each refined part.
    *   `part_1_ui_plan.txt`, `part_2_ui_plan.txt`, ...

## Configuration

The system is highly configurable through YAML files located in the `config/` directory:

*   **Agent Configuration (`config/agent/`):** Each agent (e.g., `teach_scenario_introducer.yaml`, `teach_visual_representer.yaml`) has its own configuration file defining its role, instructions, and model settings.
*   **Model Configuration (`config/model/`):** Defines parameters for the language models used (e.g., `gpt4o_openai.yaml`).
*   **Multi-Agent System Configuration (`config/multi_agents/`):** The `education.yaml` file orchestrates how different agents are grouped and sequenced for the educational query task.

## Core Components

*   **`agents/teach_agents_intermediate_system.py`:** The central class that manages the entire workflow, from receiving the user query to generating the final refined teaching plan and UI plans. It orchestrates agent calls, data flow, and output saving.
*   **`agents/base_agent.py`:** An abstract base class that provides common functionality for all agents, including loading configurations and interacting with language models.
*   **`models/openai_model.py`:** Handles communication with the OpenAI API, sending requests and receiving responses from the configured language models.

## Workflow

1.  **User Query:** The process starts with a user query (e.g., "Explain the concept of photosynthesis to a 5th grader").
2.  **Initial Plan Generation (Phase 1 & 2):** A set of initial planning agents (Scenario Introducer, Problem Solver, Schema Instruction Specialist) collaborate to generate a comprehensive initial teaching plan. This plan is structured into several parts.
3.  **Part Extraction:** The `TeachAgentsIntermediateSystem` extracts individual parts from the combined initial plan.
4.  **Iterative Refinement (Phase 3):** For each extracted part:
    *   **Discussion Agents:** Multiple specialist agents (Visual Representer, Information Expresser, Cognitive Strategist, Metacognitive Strategist) review the part and provide specific feedback.
    *   **Synthesizer Agent:** The Schema Instruction Specialist (acting as a synthesizer) takes the original part content and all the collected feedback to produce a refined version of the part.
5.  **UI Plan Generation (Phase 4):** The Web Page Formatting Specialist agent generates a UI/presentation plan for each refined part.
6.  **Output Saving:** Throughout the process, all inputs, intermediate outputs, feedback, and final results are saved to the designated output directory.

## Future Work

*   Integration with other Language Models.
*   More sophisticated agent interaction patterns.
*   User interface for easier query input and output visualization.
*   Evaluation metrics for generated content quality.

## Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues.

Special thanks to the [MDocAgent](https://github.com/aiming-lab/MDocAgent) project for providing the open-source foundation and inspiration.

